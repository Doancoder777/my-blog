# GIáº¢I THÃCH THÃ€NH PHáº¦N HUáº¤N LUYá»†N AI THEO PHÆ¯Æ NG PHÃP FEYNMAN

> **"Náº¿u báº¡n khÃ´ng thá»ƒ giáº£i thÃ­ch Ä‘iá»u gÃ¬ Ä‘Ã³ cho má»™t Ä‘á»©a tráº» 8 tuá»•i, nghÄ©a lÃ  chÃ­nh báº¡n cÅ©ng chÆ°a thá»±c sá»± hiá»ƒu nÃ³."** - Richard Feynman

<!-- HERO IMAGE: Minh há»a Train/Val/Test -->
![Train Validation Test Split](../../assets/images/placeholder-train-val-test.jpg)
*ğŸ–¼ï¸ TÃ¬m kiáº¿m: "train validation test split machine learning diagram" - SÆ¡ Ä‘á»“ chia dataset*

{{youtube:Zi-0rlM4RDs|Train Test Validation Split Explained}}

*ğŸ¥ Video: "Train/Test/Validation Split" - Giáº£i thÃ­ch trá»±c quan vá» cÃ¡ch chia dá»¯ liá»‡u trong ML*

---

## 1. Richard Feynman lÃ  ai vÃ  táº¡i sao chÃºng ta nÃªn nghe Ã´ng áº¥y?

**Richard Feynman** khÃ´ng chá»‰ lÃ  má»™t thiÃªn tÃ i váº­t lÃ½ nháº­n giáº£i Nobel, mÃ  cÃ²n Ä‘Æ°á»£c má»‡nh danh lÃ  **"The Great Explainer"** (NgÆ°á»i giáº£i thÃ­ch vÄ© Ä‘áº¡i). Ã”ng cÃ³ má»™t "dá»‹ á»©ng" Ä‘áº·c biá»‡t vá»›i nhá»¯ng thuáº­t ngá»¯ bÃ³ng báº©y nhÆ°ng rá»—ng tuáº¿ch.

Triáº¿t lÃ½ cá»§a Ã´ng ráº¥t Ä‘Æ¡n giáº£n: **"Náº¿u báº¡n khÃ´ng thá»ƒ giáº£i thÃ­ch Ä‘iá»u gÃ¬ Ä‘Ã³ cho má»™t Ä‘á»©a tráº» 8 tuá»•i, nghÄ©a lÃ  chÃ­nh báº¡n cÅ©ng chÆ°a thá»±c sá»± hiá»ƒu nÃ³."** 

Äá»‘i vá»›i má»™t ká»¹ sÆ° AIoT, phÆ°Æ¡ng phÃ¡p nÃ y chÃ­nh lÃ  "bá»™ lá»c" Ä‘á»ƒ Ä‘áº£m báº£o chÃºng ta khÃ´ng chá»‰ há»c váº¹t cÃ¡c thÃ´ng sá»‘, mÃ  thá»±c sá»± lÃ m chá»§ Ä‘Æ°á»£c logic bÃªn dÆ°á»›i. HÃ£y cÃ¹ng Ã¡p dá»¥ng cÃ¡ch nÃ y Ä‘á»ƒ hiá»ƒu vá» 3 táº­p dá»¯ liá»‡u quan trá»ng nháº¥t trong AI: **Train, Val (Validation), vÃ  Test.**

---

## 2. Cuá»™c Ä‘á»‘i thoáº¡i: Chuyá»‡n há»c hÃ nh cá»§a má»™t "SiÃªu trÃ­ tuá»‡"

HÃ£y tÆ°á»Ÿng tÆ°á»£ng **AI** lÃ  má»™t cáº­u há»c sinh, vÃ  báº¡n lÃ  **Giáº£ng viÃªn**. Má»¥c tiÃªu cá»§a chÃºng ta lÃ  chuáº©n bá»‹ cho cáº­u há»c sinh nÃ y tham gia má»™t ká»³ thi "Tháº¿ giá»›i thá»±c".

> **Há»c sinh:** "ThÆ°a tháº§y, táº¡i sao em pháº£i chia Ä‘á»‘ng dá»¯ liá»‡u nÃ y thÃ nh 3 pháº§n khÃ¡c nhau? Sao khÃ´ng Ä‘Æ°a háº¿t cho em há»c má»™t thá»ƒ cho nhanh?"

> **Giáº£ng viÃªn (Feynman):** "Äá»ƒ tháº§y ká»ƒ cho em nghe vá» cÃ¡ch má»™t ngÆ°á»i thá»±c sá»± há»c má»™t ká»¹ nÄƒng, chá»© khÃ´ng pháº£i há»c váº¹t nhÃ©."

---

### A. Táº­p dá»¯ liá»‡u TRAIN (Há»c kiáº¿n thá»©c) - "SÃ¡ch giÃ¡o khoa vÃ  BÃ i táº­p vá» nhÃ "

**Giáº£i thÃ­ch:**  
ÄÃ¢y lÃ  nhá»¯ng bÃ i toÃ¡n mÃ  tháº§y Ä‘Æ°a cho em, cÃ³ kÃ¨m theo cáº£ lá»i giáº£i chi tiáº¿t á»Ÿ cuá»‘i sÃ¡ch.

**CÃ¡ch há»c:**  
Em nhÃ¬n vÃ o Ä‘á» bÃ i, tá»± giáº£i, sau Ä‘Ã³ Ä‘á»‘i chiáº¿u vá»›i Ä‘Ã¡p Ã¡n. Náº¿u sai, em tá»± Ä‘iá»u chá»‰nh láº¡i cÃ¡ch suy nghÄ© cá»§a mÃ¬nh. Em lÃ m Ä‘i lÃ m láº¡i hÃ ng nghÃ¬n láº§n cho Ä‘áº¿n khi thuá»™c lÃ²ng cÃ¡c dáº¡ng bÃ i.

**Má»¥c tiÃªu:**  
GiÃºp AI lÃ m quen vá»›i cÃ¡c Ä‘áº·c Ä‘iá»ƒm, hÃ¬nh áº£nh hoáº·c con sá»‘ cá»¥ thá»ƒ.

**Rá»§i ro:**  
Náº¿u em chá»‰ há»c thuá»™c lÃ²ng tá»«ng con chá»¯ mÃ  khÃ´ng hiá»ƒu quy luáº­t, em sáº½ bá»‹ **"Há»c tá»§" (Overfitting)**. Gáº·p bÃ i y há»‡t thÃ¬ lÃ m Ä‘Æ°á»£c, gáº·p bÃ i khÃ¡c má»™t chÃºt lÃ  "Ä‘á»©ng hÃ¬nh".

<!-- áº¢NH: Minh há»a Overfitting vs Good Fit -->
![Overfitting vs Good Fit](../../assets/images/placeholder-overfitting.jpg)
*ğŸ–¼ï¸ TÃ¬m kiáº¿m: "overfitting vs underfitting machine learning graph" - Äá»“ thá»‹ so sÃ¡nh overfitting*

{{youtube:DEMmkFC6IGM|Overfitting Explained}}

*ğŸ¥ Video: "Overfitting in Machine Learning" - Hiá»ƒu rÃµ hiá»‡n tÆ°á»£ng overfitting*

---

### B. Táº­p dá»¯ liá»‡u VAL (Kiá»ƒm tra giá»¯a ká»³) - "BÃ i kiá»ƒm tra thá»­ trÃªn lá»›p"

**Giáº£i thÃ­ch:**  
Khi tháº§y tháº¥y em cÃ³ váº» á»•n, tháº§y cho em lÃ m má»™t bÃ i kiá»ƒm tra thá»­. Nhá»¯ng cÃ¢u há»i nÃ y em **chÆ°a tá»«ng tháº¥y** trong táº­p Train, nhÆ°ng nÃ³ cÃ³ dáº¡ng tÆ°Æ¡ng tá»±.

**CÃ¡ch há»c:**  
Em tá»± giáº£i mÃ  khÃ´ng Ä‘Æ°á»£c nhÃ¬n Ä‘Ã¡p Ã¡n. Tháº§y nhÃ¬n vÃ o Ä‘iá»ƒm sá»‘ cá»§a em Ä‘á»ƒ biáº¿t: "Ã€, cáº­u nÃ y Ä‘ang há»c quÃ¡ váº¹t" hoáº·c "Cáº­u nÃ y cáº§n táº­p trung thÃªm vÃ o pháº§n nháº­n diá»‡n hÃ¬nh áº£nh".

**Má»¥c tiÃªu:**  
DÃ¹ng káº¿t quáº£ nÃ y Ä‘á»ƒ **Ä‘iá»u chá»‰nh thÃ´ng sá»‘** (Tuning). Tháº§y sáº½ thay Ä‘á»•i cÃ¡ch dáº¡y hoáº·c báº¯t em há»c láº¡i má»™t sá»‘ chÆ°Æ¡ng.

**LÆ°u Ã½:**  
Táº­p nÃ y giÃºp chÃºng ta chá»n ra "phiÃªn báº£n" tá»‘t nháº¥t cá»§a em trÆ°á»›c khi Ä‘i thi tháº­t.

---

### C. Táº­p dá»¯ liá»‡u TEST (Ká»³ thi cuá»‘i ká»³) - "Thá»­ thÃ¡ch thá»±c táº¿"

**Giáº£i thÃ­ch:**  
ÄÃ¢y lÃ  ngÃ y thi chÃ­nh thá»©c. Nhá»¯ng cÃ¢u há»i nÃ y hoÃ n toÃ n má»›i láº¡, tháº§y vÃ  em Ä‘á»u chÆ°a tá»«ng tháº£o luáº­n vá» chÃºng trÆ°á»›c Ä‘Ã³.

**CÃ¡ch há»c:**  
ÄÃ¢y khÃ´ng pháº£i lÃ  lÃºc Ä‘á»ƒ há»c ná»¯a. ÄÃ¢y lÃ  lÃºc Ä‘á»ƒ **Ä‘Ã¡nh giÃ¡ sá»± tháº­t**.

**Má»¥c tiÃªu:**  
Äiá»ƒm sá»‘ á»Ÿ táº­p nÃ y lÃ  con sá»‘ duy nháº¥t nÃ³i lÃªn "Em cÃ³ thá»±c sá»± giá»i hay khÃ´ng?".

**Quy táº¯c tá»‘i thÆ°á»£ng:**  
Tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘Æ°á»£c cho AI "nhÃ¬n trá»™m" táº­p Test trong lÃºc há»c. Náº¿u Ä‘Ã£ nhÃ¬n tháº¥y Ä‘á» thi trÆ°á»›c khi thi, thÃ¬ má»i káº¿t quáº£ Ä‘á»u lÃ  giáº£ dá»‘i.

---

## 3. TÃ³m táº¯t cho Ká»¹ sÆ° AIoT "KhÃ³ tÃ­nh"

| ThÃ nh pháº§n | TÃªn gá»i dÃ¢n dÃ£ | Vai trÃ² ká»¹ thuáº­t |
|------------|----------------|------------------|
| **Train Set** | SÃ¡ch bÃ i táº­p | Äá»ƒ Model há»c cÃ¡c Ä‘áº·c trÆ°ng vÃ  Ä‘iá»u chá»‰nh trá»ng sá»‘ (Weights). |
| **Validation Set** | Thi thá»­ | Äá»ƒ láº­p trÃ¬nh viÃªn Ä‘iá»u chá»‰nh siÃªu tham sá»‘ (Hyperparameters). |
| **Test Set** | Thi tháº­t | Äá»ƒ Ä‘Ã¡nh giÃ¡ nÄƒng lá»±c thá»±c táº¿, khÃ´ng dÃ¹ng Ä‘á»ƒ chá»‰nh sá»­a gÃ¬ thÃªm. |

---

## 4. Ãp dá»¥ng thá»±c táº¿ trong dá»± Ã¡n AIoT

<!-- áº¢NH: Pipeline huáº¥n luyá»‡n AI -->
![AI Training Pipeline](../../assets/images/placeholder-ai-training-pipeline.jpg)
*ğŸ–¼ï¸ TÃ¬m kiáº¿m: "machine learning training pipeline diagram" - SÆ¡ Ä‘á»“ quy trÃ¬nh huáº¥n luyá»‡n AI*

{{youtube:dSCFk168vmo|Train Test Val Split Python}}

*ğŸ¥ Video: "Train Test Split in Python" - HÆ°á»›ng dáº«n chia dá»¯ liá»‡u vá»›i sklearn*

### VÃ­ dá»¥ cá»¥ thá»ƒ: Nháº­n diá»‡n rÃ¡c tháº£i vá»›i YOLOv10

Giáº£ sá»­ báº¡n Ä‘ang xÃ¢y dá»±ng há»‡ thá»‘ng phÃ¢n loáº¡i rÃ¡c tháº£i tá»± Ä‘á»™ng:

**ğŸ“š Train Set (70% dá»¯ liá»‡u):**
- 7,000 áº£nh cÃ¡c loáº¡i rÃ¡c: nhá»±a, giáº¥y, kim loáº¡i, thá»§y tinh
- AI há»c cÃ¡ch nháº­n diá»‡n Ä‘áº·c trÆ°ng cá»§a tá»«ng loáº¡i rÃ¡c
- Model Ä‘iá»u chá»‰nh weights sau má»—i epoch

**ğŸ“Š Validation Set (20% dá»¯ liá»‡u):**
- 2,000 áº£nh mÃ  AI chÆ°a tháº¥y trong Training
- Sau má»—i epoch, kiá»ƒm tra accuracy trÃªn Val Set
- Náº¿u Val loss tÄƒng mÃ  Train loss giáº£m â†’ **Overfitting!**
- Äiá»u chá»‰nh learning rate, thÃªm dropout, hoáº·c data augmentation

**ğŸ¯ Test Set (10% dá»¯ liá»‡u):**
- 1,000 áº£nh hoÃ n toÃ n má»›i, chá»¥p trong Ä‘iá»u kiá»‡n thá»±c táº¿
- Chá»‰ cháº¡y test **1 láº§n duy nháº¥t** khi Ä‘Ã£ hoÃ n táº¥t training
- Káº¿t quáº£ lÃ  con sá»‘ cuá»‘i cÃ¹ng bÃ¡o cÃ¡o vá»›i khÃ¡ch hÃ ng

---

## 5. Nhá»¯ng sai láº§m thÆ°á»ng gáº·p

### âŒ Sai láº§m 1: DÃ¹ng Test Set Ä‘á»ƒ Ä‘iá»u chá»‰nh model
```
Ká»¹ sÆ° A: "TÃ´i test tháº¥y accuracy tháº¥p, Ä‘á»ƒ tÃ´i thay Ä‘á»•i 
         learning rate rá»“i test láº¡i."
```
**Váº¥n Ä‘á»:** Báº¡n Ä‘Ã£ vÃ´ tÃ¬nh "há»c thuá»™c" Test Set, káº¿t quáº£ khÃ´ng cÃ²n khÃ¡ch quan.

**âœ… ÄÃºng:** Chá»‰ dÃ¹ng Validation Set Ä‘á»ƒ Ä‘iá»u chá»‰nh. Test Set chá»‰ cháº¡y 1 láº§n cuá»‘i cÃ¹ng.

---

### âŒ Sai láº§m 2: Val Set quÃ¡ nhá»
```
Train: 95%, Val: 5% â†’ Val Set chá»‰ cÃ³ 50 áº£nh
```
**Váº¥n Ä‘á»:** 50 áº£nh khÃ´ng Ä‘á»§ Ä‘á»ƒ Ä‘áº¡i diá»‡n cho tháº¿ giá»›i thá»±c, káº¿t quáº£ Val khÃ´ng tin cáº­y.

**âœ… ÄÃºng:** Ãt nháº¥t 15-20% cho Val Set, Ä‘áº£m báº£o Ä‘á»§ sample Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.

---

### âŒ Sai láº§m 3: Train vÃ  Val khÃ´ng cÃ¢n báº±ng
```
Train: ToÃ n áº£nh chá»¥p ban ngÃ y, Ã¡nh sÃ¡ng tá»‘t
Val: ToÃ n áº£nh chá»¥p ban Ä‘Ãªm, thiáº¿u sÃ¡ng
```
**Váº¥n Ä‘á»:** Model sáº½ tháº¥t báº¡i hoÃ n toÃ n trÃªn Val Set vÃ¬ chÆ°a há»c cÃ¡ch xá»­ lÃ½ Ä‘iá»u kiá»‡n tá»‘i.

**âœ… ÄÃºng:** Äáº£m báº£o Train/Val/Test cÃ³ phÃ¢n bá»‘ tÆ°Æ¡ng tá»± nhau vá»:
- Äiá»u kiá»‡n Ã¡nh sÃ¡ng
- GÃ³c chá»¥p
- Äá»™ phÃ¢n giáº£i
- Background

---

## 6. Lá»i káº¿t kiá»ƒu Feynman

> **"Äá»«ng cá»‘ lÃ m cho bÃ i viáº¿t cá»§a báº¡n trÃ´ng cÃ³ váº» nguy hiá»ƒm báº±ng nhá»¯ng thuáº­t ngá»¯ toÃ¡n há»c náº¿u báº¡n khÃ´ng thá»ƒ giáº£i thÃ­ch nÃ³ báº±ng má»™t vÃ­ dá»¥ Ä‘á»i thÆ°á»ng."**

---

## 7. ğŸ› ï¸ Thá»±c HÃ nh: Code HoÃ n Chá»‰nh Chia Dataset

### BÃ i toÃ¡n: PhÃ¢n loáº¡i rÃ¡c tháº£i vá»›i YOLOv8

**Cáº¥u trÃºc thÆ° má»¥c ban Ä‘áº§u:**
```
raw_data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ plastic_001.jpg
â”‚   â”œâ”€â”€ plastic_002.jpg
â”‚   â”œâ”€â”€ paper_001.jpg
â”‚   â””â”€â”€ ... (1000 áº£nh)
â””â”€â”€ labels/
    â”œâ”€â”€ plastic_001.txt
    â”œâ”€â”€ plastic_002.txt
    â””â”€â”€ ...
```

### Code Python hoÃ n chá»‰nh:

```python
"""
Script chia dataset Train/Val/Test cho YOLO
TÃ¡c giáº£: AIoT Engineer
"""

import os
import shutil
import random
from pathlib import Path
from collections import defaultdict

class DatasetSplitter:
    def __init__(self, 
                 source_dir: str, 
                 output_dir: str,
                 train_ratio: float = 0.7,
                 val_ratio: float = 0.2,
                 test_ratio: float = 0.1,
                 seed: int = 42):
        """
        Khá»Ÿi táº¡o splitter vá»›i cÃ¡c tham sá»‘
        
        Args:
            source_dir: ThÆ° má»¥c chá»©a images/ vÃ  labels/
            output_dir: ThÆ° má»¥c xuáº¥t dataset Ä‘Ã£ chia
            train_ratio: Tá»· lá»‡ Train (default 70%)
            val_ratio: Tá»· lá»‡ Validation (default 20%)
            test_ratio: Tá»· lá»‡ Test (default 10%)
            seed: Random seed Ä‘á»ƒ reproducible
        """
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
            "Tá»•ng cÃ¡c ratio pháº£i báº±ng 1.0"
        
        self.source_dir = Path(source_dir)
        self.output_dir = Path(output_dir)
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        
        random.seed(seed)
        
    def get_class_distribution(self) -> dict:
        """
        PhÃ¢n tÃ­ch phÃ¢n bá»‘ class trong dataset
        Quan trá»ng: Äáº£m báº£o má»—i split cÃ³ phÃ¢n bá»‘ tÆ°Æ¡ng tá»±
        """
        labels_dir = self.source_dir / "labels"
        class_counts = defaultdict(int)
        file_classes = {}
        
        for label_file in labels_dir.glob("*.txt"):
            with open(label_file, 'r') as f:
                classes_in_file = set()
                for line in f:
                    class_id = int(line.strip().split()[0])
                    classes_in_file.add(class_id)
                    class_counts[class_id] += 1
                file_classes[label_file.stem] = list(classes_in_file)
        
        return class_counts, file_classes
    
    def stratified_split(self, file_classes: dict) -> tuple:
        """
        Chia dataset theo stratified sampling
        Äáº£m báº£o má»—i class xuáº¥t hiá»‡n Ä‘á»u trong Train/Val/Test
        """
        # Group files by their primary class
        class_files = defaultdict(list)
        for filename, classes in file_classes.items():
            primary_class = classes[0] if classes else -1
            class_files[primary_class].append(filename)
        
        train_files = []
        val_files = []
        test_files = []
        
        # Chia tá»«ng class riÃªng biá»‡t
        for class_id, files in class_files.items():
            random.shuffle(files)
            n = len(files)
            
            n_train = int(n * self.train_ratio)
            n_val = int(n * self.val_ratio)
            
            train_files.extend(files[:n_train])
            val_files.extend(files[n_train:n_train + n_val])
            test_files.extend(files[n_train + n_val:])
        
        return train_files, val_files, test_files
    
    def copy_files(self, filenames: list, split_name: str):
        """Copy images vÃ  labels vÃ o thÆ° má»¥c tÆ°Æ¡ng á»©ng"""
        images_out = self.output_dir / split_name / "images"
        labels_out = self.output_dir / split_name / "labels"
        
        images_out.mkdir(parents=True, exist_ok=True)
        labels_out.mkdir(parents=True, exist_ok=True)
        
        for filename in filenames:
            # Copy image (thá»­ nhiá»u extension)
            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                img_src = self.source_dir / "images" / f"{filename}{ext}"
                if img_src.exists():
                    shutil.copy(img_src, images_out / f"{filename}{ext}")
                    break
            
            # Copy label
            label_src = self.source_dir / "labels" / f"{filename}.txt"
            if label_src.exists():
                shutil.copy(label_src, labels_out / f"{filename}.txt")
    
    def create_yaml_config(self, class_names: list):
        """Táº¡o file data.yaml cho YOLO"""
        yaml_content = f"""
# Dataset configuration for YOLOv8
# Auto-generated by DatasetSplitter

path: {self.output_dir.absolute()}
train: train/images
val: val/images
test: test/images

# Classes
nc: {len(class_names)}
names: {class_names}
"""
        yaml_path = self.output_dir / "data.yaml"
        with open(yaml_path, 'w') as f:
            f.write(yaml_content.strip())
        
        print(f"âœ… Created: {yaml_path}")
    
    def run(self, class_names: list):
        """Cháº¡y toÃ n bá»™ pipeline"""
        print("=" * 50)
        print("ğŸš€ Báº®T Äáº¦U CHIA DATASET")
        print("=" * 50)
        
        # 1. PhÃ¢n tÃ­ch dataset
        print("\nğŸ“Š PhÃ¢n tÃ­ch phÃ¢n bá»‘ dá»¯ liá»‡u...")
        class_counts, file_classes = self.get_class_distribution()
        
        total_samples = len(file_classes)
        print(f"   Tá»•ng sá»‘ áº£nh: {total_samples}")
        for class_id, count in sorted(class_counts.items()):
            class_name = class_names[class_id] if class_id < len(class_names) else f"class_{class_id}"
            print(f"   - {class_name}: {count} annotations")
        
        # 2. Stratified split
        print("\nâœ‚ï¸ Chia dataset (Stratified Sampling)...")
        train_files, val_files, test_files = self.stratified_split(file_classes)
        
        print(f"   Train: {len(train_files)} ({len(train_files)/total_samples*100:.1f}%)")
        print(f"   Val:   {len(val_files)} ({len(val_files)/total_samples*100:.1f}%)")
        print(f"   Test:  {len(test_files)} ({len(test_files)/total_samples*100:.1f}%)")
        
        # 3. Copy files
        print("\nğŸ“ Copying files...")
        self.copy_files(train_files, "train")
        self.copy_files(val_files, "val")
        self.copy_files(test_files, "test")
        print("   âœ… Done!")
        
        # 4. Create YAML
        print("\nğŸ“ Creating data.yaml...")
        self.create_yaml_config(class_names)
        
        # 5. Summary
        print("\n" + "=" * 50)
        print("âœ… HOÃ€N THÃ€NH!")
        print("=" * 50)
        print(f"\nğŸ“‚ Output directory: {self.output_dir}")
        print("\nğŸ”¥ Äá»ƒ train model, cháº¡y:")
        print(f"   yolo train data={self.output_dir}/data.yaml model=yolov8n.pt epochs=100")


# Sá»­ dá»¥ng
if __name__ == "__main__":
    splitter = DatasetSplitter(
        source_dir="./raw_data",
        output_dir="./trash_dataset",
        train_ratio=0.7,
        val_ratio=0.2,
        test_ratio=0.1,
        seed=42
    )
    
    class_names = ['plastic', 'paper', 'metal', 'glass', 'organic']
    splitter.run(class_names)
```

### Output khi cháº¡y:

```
==================================================
ğŸš€ Báº®T Äáº¦U CHIA DATASET
==================================================

ğŸ“Š PhÃ¢n tÃ­ch phÃ¢n bá»‘ dá»¯ liá»‡u...
   Tá»•ng sá»‘ áº£nh: 1000
   - plastic: 350 annotations
   - paper: 250 annotations
   - metal: 180 annotations
   - glass: 120 annotations
   - organic: 100 annotations

âœ‚ï¸ Chia dataset (Stratified Sampling)...
   Train: 700 (70.0%)
   Val:   200 (20.0%)
   Test:  100 (10.0%)

ğŸ“ Copying files...
   âœ… Done!

ğŸ“ Creating data.yaml...
âœ… Created: ./trash_dataset/data.yaml

==================================================
âœ… HOÃ€N THÃ€NH!
==================================================

ğŸ“‚ Output directory: ./trash_dataset

ğŸ”¥ Äá»ƒ train model, cháº¡y:
   yolo train data=./trash_dataset/data.yaml model=yolov8n.pt epochs=100
```

---

## 8. ğŸ“ˆ PhÃ¢n TÃ­ch Káº¿t Quáº£ Training

### CÃ¡ch Ä‘á»c Learning Curves

```python
import matplotlib.pyplot as plt
import pandas as pd

# Load results tá»« YOLO training
results = pd.read_csv('runs/detect/train/results.csv')

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot 1: Train vs Val Loss
ax1 = axes[0, 0]
ax1.plot(results['epoch'], results['train/box_loss'], label='Train Loss', color='blue')
ax1.plot(results['epoch'], results['val/box_loss'], label='Val Loss', color='red')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Box Loss')
ax1.set_title('ğŸ” PhÃ¡t hiá»‡n Overfitting')
ax1.legend()
ax1.grid(True)

# Annotate overfitting point
overfitting_epoch = 45  # Giáº£ sá»­
ax1.axvline(x=overfitting_epoch, color='orange', linestyle='--', label='Overfitting starts')
ax1.annotate('âš ï¸ Val loss tÄƒng\nTrain loss giáº£m\nâ†’ OVERFITTING!', 
             xy=(overfitting_epoch, 0.05), fontsize=10,
             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))

# Plot 2: mAP progression
ax2 = axes[0, 1]
ax2.plot(results['epoch'], results['metrics/mAP50(B)'], label='mAP@50', color='green')
ax2.plot(results['epoch'], results['metrics/mAP50-95(B)'], label='mAP@50-95', color='purple')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('mAP')
ax2.set_title('ğŸ“ˆ Model Improvement')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.savefig('training_analysis.png', dpi=150)
plt.show()
```

### Checklist Ä‘Ã¡nh giÃ¡ model

| Hiá»‡n tÆ°á»£ng | Cháº©n Ä‘oÃ¡n | Giáº£i phÃ¡p |
|------------|-----------|-----------|
| Train loss â†“, Val loss â†“ | âœ… Há»c tá»‘t | Tiáº¿p tá»¥c training |
| Train loss â†“, Val loss â†‘ | âš ï¸ Overfitting | Early stopping, thÃªm augmentation, dropout |
| Train loss â†’, Val loss â†’ | ğŸ˜´ Underfitting | TÄƒng model size, há»c rate, epochs |
| Train loss â†‘â†“ liÃªn tá»¥c | ğŸ¢ Learning rate quÃ¡ cao | Giáº£m learning rate |

---

## 9. ğŸ’¡ Máº¹o VÃ ng Tá»« Kinh Nghiá»‡m Thá»±c Táº¿

### Máº¹o 1: K-Fold Cross Validation cho dataset nhá»

Khi báº¡n chá»‰ cÃ³ 200-500 áº£nh, viá»‡c chia cá»‘ Ä‘á»‹nh sáº½ gÃ¢y bias. DÃ¹ng K-Fold:

```python
from sklearn.model_selection import StratifiedKFold

def kfold_split(file_classes, n_splits=5):
    """
    Chia dataset thÃ nh K folds
    Má»—i láº§n train trÃªn K-1 folds, validate trÃªn 1 fold cÃ²n láº¡i
    """
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    files = list(file_classes.keys())
    labels = [file_classes[f][0] for f in files]  # Primary class
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(files, labels)):
        train_files = [files[i] for i in train_idx]
        val_files = [files[i] for i in val_idx]
        
        print(f"Fold {fold + 1}: Train={len(train_files)}, Val={len(val_files)}")
        yield fold, train_files, val_files
```

### Máº¹o 2: Leak Detection - PhÃ¡t hiá»‡n data leak

```python
def check_data_leak(train_dir, val_dir, test_dir):
    """
    Kiá»ƒm tra xem cÃ³ áº£nh nÃ o bá»‹ trÃ¹ng giá»¯a cÃ¡c táº­p khÃ´ng
    Data leak lÃ  lá»—i nghiÃªm trá»ng!
    """
    import hashlib
    
    def get_file_hash(filepath):
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    train_hashes = {get_file_hash(f): f for f in Path(train_dir).glob("*")}
    val_hashes = {get_file_hash(f): f for f in Path(val_dir).glob("*")}
    test_hashes = {get_file_hash(f): f for f in Path(test_dir).glob("*")}
    
    # Check overlaps
    train_val_leak = set(train_hashes.keys()) & set(val_hashes.keys())
    train_test_leak = set(train_hashes.keys()) & set(test_hashes.keys())
    val_test_leak = set(val_hashes.keys()) & set(test_hashes.keys())
    
    if train_val_leak:
        print(f"ğŸš¨ LEAK! {len(train_val_leak)} áº£nh trÃ¹ng Train-Val")
    if train_test_leak:
        print(f"ğŸš¨ LEAK! {len(train_test_leak)} áº£nh trÃ¹ng Train-Test")
    if val_test_leak:
        print(f"ğŸš¨ LEAK! {len(val_test_leak)} áº£nh trÃ¹ng Val-Test")
    
    if not any([train_val_leak, train_test_leak, val_test_leak]):
        print("âœ… KhÃ´ng cÃ³ data leak!")
```

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- **"Deep Learning"** - Ian Goodfellow, Chapter 5: Machine Learning Basics
- **"Hands-On Machine Learning"** - AurÃ©lien GÃ©ron
- **Stanford CS231n:** Lecture on Training Neural Networks
- **Google ML Crash Course:** Splitting Data

---

**TÃ¡c giáº£:** AIoT Engineer  
**ChuyÃªn má»¥c:** Kiáº¿n Thá»©c  
**Cáº­p nháº­t:** 27/12/2025  
**Tags:** `AI`, `Machine Learning`, `Train Val Test`, `Dataset`, `Feynman Method`

Khi báº¡n chia nhá» dá»¯ liá»‡u Ä‘Ãºng cÃ¡ch, báº¡n khÃ´ng chá»‰ Ä‘ang huáº¥n luyá»‡n AI, báº¡n Ä‘ang xÃ¢y dá»±ng má»™t **Logic chuáº©n má»±c**.

### ğŸ¯ Checklist cho má»i dá»± Ã¡n AI:

- [ ] Train Set Ä‘á»§ lá»›n Ä‘á»ƒ model há»c Ä‘Æ°á»£c patterns (â‰¥60%)
- [ ] Validation Set Ä‘á»§ Ä‘áº¡i diá»‡n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ trung thá»±c (â‰¥15%)
- [ ] Test Set hoÃ n toÃ n Ä‘á»™c láº­p, khÃ´ng bao giá» dÃ¹ng trong quÃ¡ trÃ¬nh training (â‰¥10%)
- [ ] Cáº£ 3 táº­p Ä‘á»u cÃ³ phÃ¢n bá»‘ dá»¯ liá»‡u tÆ°Æ¡ng tá»± nhau
- [ ] KhÃ´ng bao giá» Ä‘iá»u chá»‰nh model dá»±a trÃªn káº¿t quáº£ Test Set

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- **"Surely You're Joking, Mr. Feynman!"** - Richard Feynman (Autobiography)
- **Deep Learning Specialization** - Andrew Ng (Coursera)
- **Hands-On Machine Learning** - AurÃ©lien GÃ©ron

---

**TÃ¡c giáº£:** Há»“ Äáº·ng Há»¯u Äoan - AIoT Engineer  
**ChuyÃªn má»¥c:** Kiáº¿n thá»©c AI  
**Cáº­p nháº­t:** 25/12/2025  
**Tags:** `Machine Learning`, `Deep Learning`, `Data Science`, `AI Basics`, `Feynman Method`
